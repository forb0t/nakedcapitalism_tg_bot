"""
–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π —Å —Å–∞–π—Ç–∞ Naked Capitalism
"""

import requests
import time
import json
import logging
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import sqlite3
import hashlib

class NakedCapitalismMonitor:
    def __init__(self):
        self.base_url = "https://www.nakedcapitalism.com/"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.setup_database()
        self.setup_logging()
    
    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('nakedcap_monitor.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_database(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–µ–π"""
        self.conn = sqlite3.connect('articles.db')
        cursor = self.conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                url TEXT UNIQUE NOT NULL,
                author TEXT,
                date_posted TEXT,
                content_hash TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        self.conn.commit()
    
    def get_page_content(self, url):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            return None
    
    def parse_articles(self, html_content):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        soup = BeautifulSoup(html_content, 'html.parser')
        articles = []
        
        # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –≤ —Ä–∞–∑–¥–µ–ª–µ "Recent Items"
        recent_items = soup.find('div', {'id': 'content'}) or soup.find('main')
        if recent_items:
            # –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å—Ç–∞—Ç–µ–π
            article_links = recent_items.find_all('a', href=True)
            
            for link in article_links:
                href = link.get('href')
                title = link.get_text(strip=True)
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç–µ–π (–Ω–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫)
                if (href and title and 
                    len(title) > 15 and 
                    not href.startswith('#') and
                    'nakedcapitalism.com' in href and
                    not any(skip in title.lower() for skip in ['comment', 'comments', 'older entries', '‚Üê', 'topics:', 'posted by'])):
                    
                    full_url = urljoin(self.base_url, href)
                    
                    # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∞–≤—Ç–æ—Ä–∞ –∏ –¥–∞—Ç—É
                    author = self.extract_author(link)
                    date_posted = self.extract_date(link)
                    
                    articles.append({
                        'title': title,
                        'url': full_url,
                        'author': author,
                        'date_posted': date_posted,
                        'content_hash': hashlib.md5(title.encode()).hexdigest()
                    })
        
        return articles
    
    def extract_author(self, element):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞ —Å—Ç–∞—Ç—å–∏"""
        # –ü–æ–∏—Å–∫ –∞–≤—Ç–æ—Ä–∞ –≤ —Å–æ—Å–µ–¥–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
        parent = element.parent
        if parent:
            author_elem = parent.find('span', class_='author') or parent.find('em')
            if author_elem:
                return author_elem.get_text(strip=True)
        return "Unknown"
    
    def extract_date(self, element):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã —Å—Ç–∞—Ç—å–∏"""
        parent = element.parent
        if parent:
            date_elem = parent.find('span', class_='date') or parent.find('time')
            if date_elem:
                return date_elem.get_text(strip=True)
        return datetime.now().strftime('%Y-%m-%d')
    
    def save_articles(self, articles):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        cursor = self.conn.cursor()
        new_articles = []
        
        for article in articles:
            try:
                cursor.execute('''
                    INSERT OR IGNORE INTO articles (title, url, author, date_posted, content_hash)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    article['title'],
                    article['url'],
                    article['author'],
                    article['date_posted'],
                    article['content_hash']
                ))
                
                if cursor.rowcount > 0:
                    new_articles.append(article)
                    
            except sqlite3.Error as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å—Ç–∞—Ç—å–∏: {e}")
        
        self.conn.commit()
        return new_articles
    
    def check_for_new_articles(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π"""
        self.logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π...")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        html_content = self.get_page_content(self.base_url)
        if not html_content:
            self.logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
            return []
        
        # –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π
        articles = self.parse_articles(html_content)
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        new_articles = self.save_articles(articles)
        
        if new_articles:
            self.logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(new_articles)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π:")
            for article in new_articles:
                self.logger.info(f"- {article['title']} by {article['author']}")
        else:
            self.logger.info("–ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        return new_articles
    
    def get_latest_articles(self, limit=10):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT title, url, author, date_posted, created_at
            FROM articles
            ORDER BY created_at DESC
            LIMIT ?
        ''', (limit,))
        
        return cursor.fetchall()
    
    def run_monitoring(self, interval_hours=1):
        """–ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º"""
        self.logger.info(f"–ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º {interval_hours} —á–∞—Å(–æ–≤)")
        
        try:
            while True:
                new_articles = self.check_for_new_articles()
                
                if new_articles:
                    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ—Ç–ø—Ä–∞–≤–∫—É —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
                    self.notify_new_articles(new_articles)
                
                # –û–∂–∏–¥–∞–Ω–∏–µ –¥–æ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
                sleep_seconds = interval_hours * 3600
                self.logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ {interval_hours} —á–∞—Å(–æ–≤) –¥–æ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏...")
                time.sleep(sleep_seconds)
                
        except KeyboardInterrupt:
            self.logger.info("–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–µ: {e}")
        finally:
            self.conn.close()
    
    def notify_new_articles(self, articles):
        """–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç—å—è—Ö (–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –±–æ—Ç–æ–º)"""
        self.logger.info("=== –ù–û–í–´–ï –°–¢–ê–¢–¨–ò ===")
        for article in articles:
            self.logger.info(f"üì∞ {article['title']}")
            self.logger.info(f"üë§ –ê–≤—Ç–æ—Ä: {article['author']}")
            self.logger.info(f"üîó URL: {article['url']}")
            self.logger.info("-" * 50)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    monitor = NakedCapitalismMonitor()
    
    print("üîç –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π Naked Capitalism")
    print("=" * 50)
    
    # –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    new_articles = monitor.check_for_new_articles()
    
    if new_articles:
        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(new_articles)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π!")
        for article in new_articles:
            print(f"üì∞ {article['title']} - {article['author']}")
    else:
        print("\nüìù –ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    # –ü–æ–∫–∞–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π
    print("\nüìö –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ –≤ –±–∞–∑–µ:")
    latest = monitor.get_latest_articles(5)
    for article in latest:
        print(f"- {article[0]} ({article[2]}) - {article[3]}")
    
    print("\nüöÄ –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞...")
    print("–ù–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
    
    # –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    monitor.run_monitoring(interval_hours=1)

if __name__ == "__main__":
    main()
